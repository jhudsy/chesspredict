{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chess\n",
    "import chess.pgn\n",
    "from chess.engine import PovScore, Cp\n",
    "from io import StringIO,TextIOWrapper\n",
    "import h5py\n",
    "import sys,os\n",
    "import zstandard as zstd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the data in several stages.\n",
    "\n",
    "1. Extract valid games (see below) into pgn files using string operations.\n",
    "2. Create different hdf5 files for training, validation and testing containing (game,label) tensors for different time controls from multiple pgn files, filtering out invalid games using the python chess library.\n",
    "3. Create oversampled training files.\n",
    "\n",
    "Valid game files are based on the `TimeControl \"{a time control}\"` field, whether the term `[%eval` is in the string and whether `BlackRatingDiff` and `WhiteRatingDiff` is below a certain value as well as whether `Termination` is *not* `Abandoned` or `Rules nfraction`.\n",
    "\n",
    "If all the above conditions are met we utilise the `Board` class to parse the pgn, also checking that game length is above some minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stores a time-control:file string. The time-control is a regex string that matches the time-control of a game. The file string is the name of the file that the game should be saved to.\n",
    "\n",
    "file_dict = {\"300+0\":\"blitz\",\n",
    "             \"300+3\":\"blitz\",\n",
    "             \"60+0\":\"ultrabullet\",\n",
    "             \"120+1\":\"bullet\",\n",
    "             \"180+0\":\"superblitz\",\n",
    "             \"180+2\":\"superblitz\",\n",
    "             \"600+0\":\"rapid\",\n",
    "             \"600+5\":\"rapid\",\n",
    "             \"900+10\":\"rapid\"\n",
    "}\n",
    "\n",
    "#the maximum rating diff above which we ignore the game\n",
    "MAX_RATING_DIFF = 40\n",
    "\n",
    "#Termination strings that we ignore\n",
    "TERMINATION_STRINGS=set([\"Abandoned\",\"Rules infraction\"])\n",
    "\n",
    "NUM_MOVES = 40 #number of moves to consider for each game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_game_tensor(game_string):\n",
    "    \"\"\"returns a tensor representation of the game string. If the game is invalid, returns None. Note that a valid game will have 2 game tensors, one for each player. We also return the ratings of the players and the file that the game should be saved to.\"\"\"\n",
    "\n",
    "    #start by checking if the game is valid. The time control is a substring of the form 'TimeControl \"{TC}\"' where {TC} is a variable, check if {TC} is in the file_dict.\n",
    "\n",
    "    time_control = game_string.split('TimeControl \"')[1].split('\"')[0]\n",
    "    if time_control not in file_dict:\n",
    "        return None\n",
    "    \n",
    "    valid = False\n",
    "    \n",
    "    if '[%eval' in game_string and 'WhiteRatingDiff' in game_string and 'BlackRatingDiff' in game_string:\n",
    "        white_diff = int(game_string.split('WhiteRatingDiff \"')[1].split('\"')[0])\n",
    "        black_diff = int(game_string.split('BlackRatingDiff \"')[1].split('\"')[0])\n",
    "        if abs(white_diff) < MAX_RATING_DIFF and abs(black_diff) < MAX_RATING_DIFF:\n",
    "            valid = True\n",
    "    if not valid:\n",
    "        return None\n",
    "    \n",
    "    #check for termination strings\n",
    "    for term in TERMINATION_STRINGS:\n",
    "        if term in game_string:\n",
    "            return None\n",
    "    \n",
    "    ########prepare the game tensors\n",
    "    gt1 = np.zeros((NUM_MOVES,136),dtype=np.int16)\n",
    "    gt2 = np.zeros((NUM_MOVES,136),dtype=np.int16)\n",
    "\n",
    "    game = chess.pgn.read_game(StringIO(game_string))\n",
    "\n",
    "    board = game.board()\n",
    "    white_time = 0\n",
    "    black_time = 0\n",
    "\n",
    "    move_number = 0\n",
    "\n",
    "    current_eval = PovScore(Cp(0), chess.WHITE)\n",
    "    current_move_color = chess.WHITE\n",
    "    while True:\n",
    "        t = np.zeros(136)\n",
    "\n",
    "        for i in range(64):\n",
    "            if board.piece_at(i) is None:\n",
    "                t[i] = 0\n",
    "            elif board.piece_at(i).color == current_move_color:\n",
    "                t[i] = board.piece_at(i).piece_type\n",
    "            else:\n",
    "                t[i] = board.piece_at(i).piece_type + 7\n",
    "\n",
    "        # get the evaluation, time etc.\n",
    "        t[128] = move_number // 2  # move number\n",
    "\n",
    "        t[129] = white_time if current_move_color == chess.WHITE else black_time\n",
    "\n",
    "        t[131] = black_time if current_move_color == chess.WHITE else white_time\n",
    "\n",
    "        if current_eval is None: #mate in 0\n",
    "            t[135] = 1\n",
    "            t[134] = 0\n",
    "        elif current_eval.pov(current_move_color).is_mate(): #mate in X\n",
    "            t[133] = 1\n",
    "            t[132] = current_eval.pov(current_move_color).mate()\n",
    "        else:\n",
    "            t[133] = 0\n",
    "            t[132] = current_eval.pov(current_move_color).score()\n",
    "\n",
    "        if move_number == 0:\n",
    "            m = game.next()\n",
    "        else:\n",
    "            m = m.next()\n",
    "        if m is None:\n",
    "            break\n",
    "\n",
    "        if current_move_color == chess.WHITE:\n",
    "            white_time = m.clock()\n",
    "        else:\n",
    "            black_time = m.clock()\n",
    "\n",
    "        current_eval = m.eval()\n",
    "        board = m.board()\n",
    "\n",
    "        for i in range(64):\n",
    "            if board.piece_at(i) is None:\n",
    "                t[i + 64] = 0\n",
    "            elif board.piece_at(i).color == current_move_color:\n",
    "                t[i + 64] = board.piece_at(i).piece_type\n",
    "            else:\n",
    "                t[i + 64] = board.piece_at(i).piece_type + 7\n",
    "\n",
    "        t[130] = white_time if current_move_color == chess.WHITE else black_time\n",
    "\n",
    "        if current_eval is None:\n",
    "            t[135] = 1\n",
    "            t[134] = 0\n",
    "        elif current_eval.pov(current_move_color).is_mate():\n",
    "            t[135] = 1\n",
    "            t[134] = current_eval.pov(current_move_color).mate()\n",
    "        else:\n",
    "            t[135] = 0\n",
    "            t[134] = current_eval.pov(current_move_color).score()\n",
    "\n",
    "        if current_move_color == chess.WHITE:\n",
    "            gt1[move_number // 2] = t\n",
    "        else:\n",
    "            gt2[move_number // 2] = t\n",
    "\n",
    "        current_move_color = not current_move_color\n",
    "\n",
    "        move_number += 1\n",
    "\n",
    "        if move_number == NUM_MOVES * 2:\n",
    "            break\n",
    "\n",
    "    return np.array(gt1),np.array(gt2),int(game.headers['WhiteElo']),int(game.headers['BlackElo']),file_dict[time_control]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to read from an input file compressed using zst and write all the resultant game tensors and ratings to a hdf5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKSIZE = 1000\n",
    "\n",
    "def write_to_hdf5(reader):\n",
    "    \"\"\"writes the games in the reader to an hdf5 file. The reader is a generator that yields game strings. The games are stored in the file according to the time-control of the game. We will write the game tensors as a dataset in the file. We will also write the ratings of the players as a dataset in the file. The file will be named according to the time-control of the games.\"\"\"\n",
    "\n",
    "    #open all the files so that we don't have to keep doing it.\n",
    "    files = {}\n",
    "    for file_name in set(file_dict.values()):\n",
    "        files[file_name] = h5py.File(f\"{file_name}.hdf5\",\"a\") #5*10^8 bytes = 500MB for the cache for each file\n",
    "\n",
    "    file_indexes = {}\n",
    "    if files[file_name].get(\"game_tensors\") is not None:\n",
    "        file_indexes = {file_name:len(files[file_name][\"game_tensors\"]) for file_name in files}\n",
    "    else:\n",
    "        file_indexes = {file_name:0 for file_name in files}\n",
    "\n",
    "    game = \"\"\n",
    "    count = 0\n",
    "\n",
    "    for line in reader:\n",
    "        if line.startswith(\"[Event\") and game == \"\": #start of a new game when the file hasn't been initialized\n",
    "            game = line\n",
    "        elif line.startswith(\"[Event\") and game != \"\": #start of a new game when the file has been initialized, write the previous game to the file\n",
    "\n",
    "            if count % 1000 == 0:\n",
    "                print(\"read\",count,\"games\")\n",
    "            count += 1\n",
    "\n",
    "            game_tensors = get_game_tensor(game)\n",
    "            if game_tensors is None:\n",
    "                game = line\n",
    "                continue\n",
    "            else:\n",
    "                #print(\"read game\",game)\n",
    "                \n",
    "                gt1,gt2,white_rating,black_rating,file_name = game_tensors\n",
    "                #print(np.array(gt1.shape),np.array([white_rating]).shape)\n",
    "                f = files[file_name]\n",
    "                if f.get(\"game_tensors\") is None:\n",
    "                    f.create_dataset(\"game_tensors\",shape=(CHUNKSIZE,40,136),maxshape=(None,40,136),chunks=True,compression='lzf')#,compression_opts=1)\n",
    "                    f.create_dataset(\"ratings\",shape=(CHUNKSIZE,1),chunks=True,maxshape=(None,1))#,compression='gzip',compression_opts=9)\n",
    "                    f[\"game_tensors\"][0] = gt1\n",
    "                    f[\"game_tensors\"][1] = gt2\n",
    "                    f[\"ratings\"][0] = np.array([white_rating])\n",
    "                    f[\"ratings\"][1] = np.array([black_rating])\n",
    "                    file_indexes[file_name] = 2\n",
    "                else: #file already exists\n",
    "                    #check if we need to resize the dataset\n",
    "                    if file_indexes[file_name]+1 >= f[\"game_tensors\"].shape[0]:\n",
    "                        print(\"enlarging chunk for file\",file_name)\n",
    "                    #+1 as we are writing 2 games at a time\n",
    "                        f[\"game_tensors\"].resize((f[\"game_tensors\"].shape[0] + CHUNKSIZE,40,136))\n",
    "                        f[\"ratings\"].resize((f[\"ratings\"].shape[0] + CHUNKSIZE,1))\n",
    "                    #write the new game\n",
    "                    f[\"game_tensors\"][file_indexes[file_name]] = gt1\n",
    "                    f[\"game_tensors\"][file_indexes[file_name]+1] = gt2\n",
    "                    f[\"ratings\"][file_indexes[file_name]] = np.array([white_rating])\n",
    "                    f[\"ratings\"][file_indexes[file_name]+1] = np.array([black_rating])\n",
    "                    file_indexes[file_name] += 2\n",
    "                game = line\n",
    "        else: #continue reading the game\n",
    "            game += line\n",
    "\n",
    "    for file_name in files:\n",
    "        f = files[file_name]\n",
    "        #reshape the datasets to remove the extra space\n",
    "        f[\"game_tensors\"].resize((file_indexes[f],40,136))\n",
    "        f[\"ratings\"].resize((file_indexes[f],1))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow processing of \"plain\" files or \"zst\" files passed in from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(fn):\n",
    "    #if the filename ends in .pgn we will read it as a text file. If it ends in .zst we will read it as a compressed file using streaming.\n",
    "    if fn.endswith(\".pgn\"):\n",
    "        with open(fn,\"r\") as f:\n",
    "            write_to_hdf5(f)\n",
    "    elif fn.endswith(\".zst\"):\n",
    "        with open(fn,\"rb\") as f:\n",
    "            dctx = zstd.ZstdDecompressor()\n",
    "            with dctx.stream_reader(f) as reader:\n",
    "                text_stream = TextIOWrapper(reader, encoding='utf-8')\n",
    "                write_to_hdf5(text_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read_file(\"data/all_data/lichess09.pgn.zst\")\n",
    "read_file(\"data/all_data/lichess05.pgn.zst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_bins(f,start_index,end_index,min_rating,max_rating,path): \n",
    "    #read the hdf file up to some index and bins index values into num_bins bins based on the rating. These bins are in intervals of 50. We will store the data in the bins in separate files in the path directory.\n",
    "    #N.B., f is a h5py file object. We will create a set of files under path containing the data in the bins.\n",
    "\n",
    "    #if the path doesn't exist, create it\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    ratings = f[\"ratings\"][start_index:end_index]\n",
    "    \n",
    "    num_bins = int((max_rating//50)-(min_rating//50))\n",
    "    start_bin_rating = (min_rating//50)*50\n",
    "\n",
    "    files = [h5py.File(f\"{path}/bin_{i}.hdf5\",\"w\") for i in range(num_bins)]\n",
    "    for fl in files:\n",
    "        fl.create_dataset(\"game_tensors\",shape=(0,40,136),maxshape=(None,40,136),compression='lzf',chunks=True)\n",
    "        fl.create_dataset(\"ratings\",shape=(0,1),maxshape=(None,1),chunks=True)\n",
    "\n",
    "    for i in range(len(ratings)):\n",
    "        bin=0\n",
    "        r=f[\"ratings\"][i][0]\n",
    "        if r<=min_rating:\n",
    "            bin=0\n",
    "        elif r>=max_rating:\n",
    "            bin=num_bins-1\n",
    "        else:\n",
    "            bin=int((r-start_bin_rating)//50)\n",
    "        files[bin][\"game_tensors\"].resize((files[bin][\"game_tensors\"].shape[0]+1,40,136))\n",
    "        files[bin][\"ratings\"].resize((files[bin][\"ratings\"].shape[0]+1,1))\n",
    "        files[bin][\"game_tensors\"][-1] = f[\"game_tensors\"][i]\n",
    "        files[bin][\"ratings\"][-1] = f[\"ratings\"][i]\n",
    "        if i%100==0:\n",
    "            print(f\"done {i}\")\n",
    "    \n",
    "    for fl in files:\n",
    "        fl.close()\n",
    "\n",
    "####################################################\n",
    "#split_file(ORIGDATA,TESTDATA,int(num_tensors*(split[0]+split[1]),num_tensors))\n",
    "def split_file(original_file_path,new_file_path,start_index,end_index):\n",
    "    #splits the original file by extracting the data from start_index to end_index and writing it to a new file.\n",
    "    with h5py.File(original_file_path,\"r\") as f:\n",
    "        with h5py.File(new_file_path,\"w\") as nf:\n",
    "            nf.create_dataset(\"game_tensors\",data=f[\"game_tensors\"][start_index:end_index])\n",
    "            nf.create_dataset(\"ratings\",data=f[\"ratings\"][start_index:end_index])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-04 08:36:28.394030: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import random\n",
    "\n",
    "class InMemoryOverSamplngGenerator(Sequence):\n",
    "    def __init__(self,path,batch_size,**kwargs):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = kwargs.get(\"shuffle\",True)\n",
    "        self.num_items = kwargs.get(\"num_items\",None)\n",
    "\n",
    "        self.files = [h5py.File(f\"{path}/bin_{i}.hdf5\",\"r\") for i in range(len(os.listdir(path))]\n",
    "                      \n",
    "        self.bins = [[] for i in range(len(self.files))]\n",
    "        self.num_bins = len(self.bins)\n",
    "\n",
    "        #read the data into memory\n",
    "        for i in range(len(self.files)):\n",
    "            self.bins[i] = (self.files[i][\"game_tensors\"][:],self.files[i][\"ratings\"][:])\n",
    "        \n",
    "        self.current_bin = 0\n",
    "        self.bin_indexes = [0 for i in range(self.num_bins)]\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.num_items is None:\n",
    "            return sum([len(b[0]) for b in self.bins])//self.batch_size\n",
    "        else:\n",
    "            return self.num_items//self.batch_size\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        num_items = self.batch_size\n",
    "        while num_items > 0:\n",
    "            if self.bin_indexes[self.current_bin] == len(self.bins[self.current_bin][0]):\n",
    "                self.bin_indexes[self.current_bin] = 0\n",
    "                self.current_bin += 1\n",
    "                self.current_bin %= self.num_bins\n",
    "\n",
    "            x_batch.append(self.bins[self.current_bin][0][self.bin_indexes[self.current_bin]])\n",
    "            y_batch.append(self.bins[self.current_bin][1][self.bin_indexes[self.current_bin]])\n",
    "\n",
    "            self.bin_indexes[self.current_bin] += 1\n",
    "            num_items -= 1\n",
    "\n",
    "        return np.array(x_batch),np.array(y_batch)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            for i in range(self.num_bins):\n",
    "                state = random.get_state()\n",
    "                seed = random.randint(0,10000)\n",
    "                random.seed(seed)\n",
    "                random.shuffle(self.bins[i][0])\n",
    "                random.seed(seed)\n",
    "                random.shuffle(self.bins[i][1])\n",
    "                random.set_state(state)\n",
    "    \n",
    "\n",
    "\n",
    "class TrainingGenerator(Sequence):\n",
    "    \"\"\"This generator takes in a path containing a set of hdf5 files, each of which is a bin. The generator will yield data by taking batch_size elements from each bin in the path. We will store cache_size elements from each file in memory, loading them in as needed. The generator will load the data from the files in the path in order, and will loop back to the start when it reaches the end of the files. The generator will also shuffle the order of the files if shuffle is set to True.\"\"\"\n",
    "    def __init__(self,path,batch_size,**kwargs):\n",
    "                \n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = kwargs.get(\"shuffle\",True)\n",
    "        self.cache_size = kwargs.get(\"cache_size\",128)\n",
    "        self.num_items = kwargs.get(\"num_items\",None)\n",
    "        \n",
    "\n",
    "        self.files = [h5py.File(f\"{path}/bin_{i}.hdf5\",\"a\") for i in range(len(os.listdir(path)))]\n",
    "        self.num_files = len(self.files)\n",
    "        self.file_indexes = [0 for i in range(self.num_files)]\n",
    "        self.game_cache = [np.zeros((self.cache_size,40,136),dtype=np.int16) for i in range(self.num_files)]\n",
    "        self.rating_cache = [np.zeros((self.cache_size,1),dtype=np.int16) for i in range(self.num_files)]\n",
    "\n",
    "        self.cache_index = [0 for i in range(self.num_files)]\n",
    "\n",
    "        self.current_file = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"returns the number of batches in the generator. This is the sum of the number of elements in each file divided by the batch size.\"\"\"\n",
    "        if self.num_items is None:\n",
    "            return sum([len(f[\"ratings\"]) for f in self.files])//self.batch_size\n",
    "        else:\n",
    "            return self.num_items//self.batch_size\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        \"\"\"returns the next batch. The batch is a tuple containing the game tensors and the ratings. We ignore the index as we will just iterate through the files in order.\"\"\"\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        num_items = self.batch_size\n",
    "\n",
    "        while num_items>0:\n",
    "            #check if we need to load more data into the cache\n",
    "            if self.cache_index[self.current_file] == 0:\n",
    "                self.__load_data(self.current_file)\n",
    "\n",
    "            #get the next element from the cache\n",
    "            x_batch.append(self.game_cache[self.current_file][self.cache_index[self.current_file]])\n",
    "            y_batch.append(self.rating_cache[self.current_file][self.cache_index[self.current_file]])\n",
    "\n",
    "            self.cache_index[self.current_file] += 1\n",
    "            self.cache_index[self.current_file] %= self.cache_size\n",
    "            self.current_file += 1\n",
    "            self.current_file %= self.num_files\n",
    "            num_items -= 1\n",
    "\n",
    "        return np.array(x_batch),np.array(y_batch)\n",
    "    \n",
    "    def __load_data(self,file_index):\n",
    "        \"\"\"loads the next cache_size elements from the file at file_index into the cache. If the end of the file is reached, the cache loops around. \"\"\"\n",
    "        f = self.files[file_index]\n",
    "    \n",
    "        start_index = self.file_indexes[file_index]\n",
    "        end_index = start_index+self.cache_size\n",
    "        if end_index >= len(f[\"ratings\"]):\n",
    "            end_index = len(f[\"ratings\"])\n",
    "            self.file_indexes[file_index] = 0\n",
    "        self.game_cache[file_index] = f[\"game_tensors\"][start_index:end_index]\n",
    "        self.rating_cache[file_index] = f[\"ratings\"][start_index:end_index]\n",
    "        self.file_indexes[file_index] = end_index\n",
    "\n",
    "        #if we have not loaded enough data, load more\n",
    "        if end_index-start_index < self.cache_size:\n",
    "            self.__load_data(file_index)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"shuffles the order of the files if shuffle is set to True. Also cleares the caches and resets the file indexes.\"\"\"\n",
    "        self.file_indexes = [0 for i in range(self.num_files)]\n",
    "        self.game_cache = [np.zeros((self.cache_size,40,136),dtype=np.int16) for i in range(self.num_files)]\n",
    "        self.rating_cache = [np.zeros((self.cache_size,1),dtype=np.int16) for i in range(self.num_files)]\n",
    "        self.cache_index = [0 for i in range(self.num_files)]\n",
    "        self.current_file = 0\n",
    "\n",
    "        if self.shuffle:\n",
    "            print(\"shuffling files\")    \n",
    "            #save current random number generator state\n",
    "            prng_state = random.get_state()\n",
    "            \n",
    "            for f in self.files:\n",
    "                print(\"shuffling file \",f.filename)\n",
    "                seed = random.randint(0,10000)\n",
    "                random.seed(seed)\n",
    "                random.shuffle(f[\"ratings\"])\n",
    "                random.seed(seed)\n",
    "                random.shuffle(f[\"game_tensors\"])\n",
    "            \n",
    "            #restore the random number generator state\n",
    "            random.set_state(prng_state)\n",
    "            print(\"done shuffling files\")\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        for f in self.files:\n",
    "            f.close()\n",
    "\n",
    "class InMemoryGenerator(Sequence):\n",
    "    def __init__(self,file,batch_size,shuffle=False):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle=shuffle\n",
    "\n",
    "        with h5py.File(file,\"r\") as f:\n",
    "            self.game_tensors = f[\"game_tensors\"][:]\n",
    "            self.ratings = f[\"ratings\"][:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)//self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "        x_batch=[]\n",
    "        y_batch=[]\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            x_batch.append(self.game_tensors[(index*self.batch_size+i)%len(self.game_tensors)])\n",
    "            y_batch.append(self.ratings[(index*self.batch_size+i)%len(self.ratings)])\n",
    "        \n",
    "        return np.array(x_batch),np.array(y_batch)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            state = random.get_state()\n",
    "            seed = random.randint(0,10000)\n",
    "            random.seed(seed)\n",
    "            random.shuffle(self.game_tensors)\n",
    "            random.seed(seed)\n",
    "            random.shuffle(self.ratings)\n",
    "            random.set_state(state)\n",
    "\n",
    "        \n",
    "class HDF5FileGenerator(Sequence):\n",
    "    def __init__(self, file, batch_size, shuffle=False):\n",
    "        self.f = h5py.File(file,\"a\",rdcc_nbytes=5*10**8) #500MB cache\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle=shuffle\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.f[\"ratings\"]))//self.batch_size\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        x_batch=[]\n",
    "        y_batch=[]\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            x_batch.append(self.f[\"game_tensors\"][(index*self.batch_size+i)%len(self.f[\"game_tensors\"])])\n",
    "            y_batch.append(self.f[\"ratings\"][(index*self.batch_size+i)%len(self.f[\"ratings\"])])\n",
    "        \n",
    "        return np.array(x_batch),np.array(y_batch)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            state = random.get_state()\n",
    "            seed = random.randint(0,10000)\n",
    "            random.seed(seed)\n",
    "            random.shuffle(self.f[\"game_tensors\"])\n",
    "            random.seed(seed)\n",
    "            random.shuffle(self.f[\"ratings\"])\n",
    "            random.set_state(state)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "inputs = Input(shape=(NUM_MOVES, 136)) #full tensor\n",
    "\n",
    "#make a dense layer for each of the NUM_MOVES elements. The output of each dense layer is a 1D tensor of 137 elements. Each of these tensors is then concatenated to form a 2D tensor of 137xNUM_MOVES elements. This tensor is then fed into an LSTM layer.\n",
    "\n",
    "x = TimeDistributed(Dense(80,activation = 'relu'))(inputs)\n",
    "\n",
    "x = LSTM(40,return_sequences = True)(x)\n",
    "x = LSTM(32)(x)\n",
    "#x = LSTM(40)(x)\n",
    "x = Dense(60,activation='relu')(x)\n",
    "\n",
    "output = Dense(1,activation='relu',name=\"Elo\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs,outputs=[output])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                    loss={'Elo':'mae'},\n",
    "                    metrics={'Elo':'mae'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 0\n",
      "done 100\n",
      "done 200\n",
      "done 300\n",
      "done 400\n",
      "done 500\n",
      "done 600\n",
      "done 700\n",
      "done 800\n",
      "done 900\n",
      "done 1000\n",
      "done 1100\n",
      "done 1200\n",
      "done 1300\n",
      "done 1400\n",
      "done 1500\n",
      "done 1600\n",
      "done 1700\n",
      "done 1800\n",
      "done 1900\n",
      "done 2000\n",
      "done 2100\n",
      "done 2200\n",
      "done 2300\n",
      "done 2400\n",
      "done 2500\n",
      "done 2600\n",
      "done 2700\n",
      "done 2800\n",
      "done 2900\n",
      "done 3000\n",
      "done 3100\n",
      "done 3200\n",
      "done 3300\n",
      "done 3400\n",
      "done 3500\n",
      "done 3600\n",
      "done 3700\n",
      "done 3800\n",
      "done 3900\n",
      "done 4000\n",
      "done 4100\n",
      "done 4200\n",
      "done 4300\n",
      "done 4400\n",
      "done 4500\n",
      "done 4600\n",
      "done 4700\n",
      "done 4800\n",
      "done 4900\n",
      "done 5000\n",
      "done 5100\n",
      "done 5200\n",
      "done 5300\n",
      "done 5400\n",
      "done 5500\n",
      "done 5600\n",
      "done 5700\n",
      "done 5800\n",
      "done 5900\n",
      "done 6000\n",
      "done 6100\n",
      "done 6200\n",
      "done 6300\n",
      "done 6400\n",
      "done 6500\n",
      "done 6600\n",
      "done 6700\n",
      "done 6800\n",
      "done 6900\n",
      "done 7000\n",
      "done 7100\n",
      "done 7200\n",
      "done 7300\n",
      "done 7400\n",
      "done 7500\n",
      "done 7600\n",
      "done 7700\n",
      "done 7800\n",
      "done 7900\n",
      "done 8000\n",
      "done 8100\n",
      "done 8200\n",
      "done 8300\n",
      "done 8400\n",
      "done 8500\n",
      "done 8600\n",
      "done 8700\n",
      "done 8800\n",
      "done 8900\n",
      "done 9000\n",
      "done 9100\n",
      "done 9200\n",
      "done 9300\n",
      "done 9400\n",
      "done 9500\n",
      "done 9600\n",
      "done 9700\n",
      "done 9800\n",
      "done 9900\n",
      "done 10000\n",
      "done 10100\n",
      "done 10200\n",
      "done 10300\n",
      "done 10400\n",
      "done 10500\n",
      "done 10600\n",
      "done 10700\n",
      "done 10800\n",
      "done 10900\n",
      "done 11000\n",
      "done 11100\n",
      "done 11200\n",
      "done 11300\n",
      "done 11400\n",
      "done 11500\n",
      "done 11600\n",
      "done 11700\n",
      "done 11800\n",
      "done 11900\n",
      "done 12000\n",
      "done 12100\n",
      "done 12200\n",
      "done 12300\n",
      "done 12400\n",
      "done 12500\n",
      "done 12600\n",
      "done 12700\n",
      "done 12800\n",
      "done 12900\n",
      "done 13000\n",
      "done 13100\n",
      "done 13200\n",
      "done 13300\n",
      "done 13400\n",
      "done 13500\n",
      "done 13600\n",
      "done 13700\n",
      "done 13800\n",
      "done 13900\n",
      "done 14000\n",
      "done 14100\n",
      "done 14200\n",
      "done 14300\n",
      "done 14400\n",
      "done 14500\n",
      "done 14600\n",
      "done 14700\n",
      "done 14800\n",
      "done 14900\n",
      "done 15000\n",
      "done 15100\n",
      "done 15200\n",
      "done 15300\n",
      "done 15400\n",
      "done 15500\n",
      "done 15600\n",
      "done 15700\n",
      "done 15800\n",
      "done 15900\n",
      "done 16000\n",
      "done 16100\n",
      "done 16200\n",
      "done 16300\n",
      "done 16400\n",
      "done 16500\n",
      "done 16600\n",
      "done 16700\n",
      "done 16800\n",
      "done 16900\n",
      "done 17000\n",
      "done 17100\n",
      "done 17200\n",
      "done 17300\n",
      "done 17400\n",
      "done 17500\n",
      "done 17600\n",
      "done 17700\n",
      "done 17800\n",
      "done 17900\n",
      "done 18000\n",
      "done 18100\n",
      "done 18200\n",
      "done 18300\n",
      "done 18400\n",
      "done 18500\n",
      "done 18600\n",
      "done 18700\n",
      "done 18800\n",
      "done 18900\n",
      "done 19000\n",
      "done 19100\n",
      "done 19200\n",
      "done 19300\n",
      "done 19400\n",
      "done 19500\n",
      "done 19600\n",
      "done 19700\n",
      "done 19800\n",
      "done 19900\n",
      "done 20000\n",
      "done 20100\n",
      "done 20200\n",
      "done 20300\n",
      "done 20400\n",
      "done 20500\n",
      "done 20600\n",
      "done 20700\n",
      "done 20800\n",
      "done 20900\n",
      "done 21000\n",
      "done 21100\n",
      "done 21200\n",
      "done 21300\n",
      "done 21400\n",
      "done 21500\n",
      "done 21600\n",
      "done 21700\n",
      "done 21800\n",
      "done 21900\n",
      "done 22000\n",
      "done 22100\n",
      "done 22200\n",
      "done 22300\n",
      "done 22400\n",
      "done 22500\n",
      "done 22600\n",
      "done 22700\n",
      "done 22800\n",
      "done 22900\n",
      "done 23000\n",
      "done 23100\n",
      "done 23200\n",
      "done 23300\n",
      "done 23400\n",
      "done 23500\n",
      "done 23600\n",
      "done 23700\n",
      "done 23800\n",
      "done 23900\n",
      "done 24000\n",
      "done 24100\n",
      "done 24200\n",
      "done 24300\n",
      "done 24400\n",
      "done 24500\n",
      "done 24600\n",
      "done 24700\n",
      "done 24800\n",
      "done 24900\n",
      "done 25000\n",
      "done 25100\n",
      "done 25200\n",
      "done 25300\n",
      "done 25400\n",
      "done 25500\n",
      "done 25600\n",
      "done 25700\n",
      "done 25800\n",
      "done 25900\n",
      "done 26000\n",
      "done 26100\n",
      "done 26200\n",
      "done 26300\n",
      "done 26400\n",
      "done 26500\n",
      "done 26600\n",
      "done 26700\n",
      "done 26800\n",
      "done 26900\n",
      "done 27000\n",
      "done 27100\n",
      "done 27200\n",
      "done 27300\n",
      "done 27400\n",
      "done 27500\n",
      "done 27600\n",
      "done 27700\n",
      "done 27800\n",
      "done 27900\n",
      "done 28000\n",
      "done 28100\n",
      "done 28200\n",
      "done 28300\n",
      "done 28400\n",
      "done 28500\n",
      "done 28600\n",
      "done 28700\n",
      "done 28800\n",
      "done 28900\n",
      "done 29000\n",
      "done 29100\n",
      "done 29200\n",
      "done 29300\n",
      "done 29400\n",
      "done 29500\n",
      "done 29600\n",
      "done 29700\n",
      "done 29800\n",
      "done 29900\n",
      "done 30000\n",
      "done 30100\n",
      "done 30200\n",
      "done 30300\n",
      "done 30400\n",
      "done 30500\n",
      "done 30600\n",
      "done 30700\n",
      "done 30800\n",
      "done 30900\n",
      "done 31000\n",
      "done 31100\n",
      "done 31200\n",
      "done 31300\n",
      "done 31400\n",
      "done 31500\n",
      "done 31600\n",
      "done 31700\n",
      "done 31800\n",
      "done 31900\n",
      "done 32000\n",
      "done 32100\n",
      "done 32200\n",
      "done 32300\n",
      "done 32400\n",
      "done 32500\n",
      "done 32600\n",
      "done 32700\n",
      "done 32800\n",
      "done 32900\n",
      "done 33000\n",
      "done 33100\n",
      "done 33200\n",
      "done 33300\n",
      "done 33400\n",
      "done 33500\n",
      "done 33600\n",
      "done 33700\n",
      "done 33800\n",
      "done 33900\n",
      "done 34000\n",
      "done 34100\n",
      "done 34200\n",
      "done 34300\n",
      "done 34400\n",
      "done 34500\n",
      "done 34600\n",
      "done 34700\n",
      "done 34800\n",
      "done 34900\n",
      "done 35000\n",
      "done 35100\n",
      "done 35200\n",
      "done 35300\n",
      "done 35400\n",
      "done 35500\n",
      "done 35600\n",
      "done 35700\n",
      "done 35800\n",
      "done 35900\n",
      "done 36000\n",
      "done 36100\n",
      "done 36200\n",
      "done 36300\n",
      "done 36400\n",
      "done 36500\n",
      "done 36600\n",
      "done 36700\n",
      "done 36800\n",
      "done 36900\n",
      "done 37000\n",
      "done 37100\n",
      "done 37200\n",
      "done 37300\n",
      "done 37400\n",
      "done 37500\n",
      "done 37600\n",
      "done 37700\n",
      "done 37800\n",
      "done 37900\n",
      "done 38000\n",
      "done 38100\n",
      "done 38200\n",
      "done 38300\n",
      "done 38400\n",
      "done 38500\n",
      "done 38600\n",
      "done 38700\n",
      "done 38800\n",
      "done 38900\n",
      "done 39000\n",
      "done 39100\n",
      "done 39200\n",
      "done 39300\n",
      "done 39400\n",
      "done 39500\n",
      "done 39600\n",
      "done 39700\n",
      "done 39800\n",
      "done 39900\n",
      "done 40000\n",
      "done 40100\n",
      "done 40200\n",
      "done 40300\n",
      "done 40400\n",
      "done 40500\n",
      "done 40600\n",
      "done 40700\n",
      "done 40800\n",
      "done 40900\n",
      "done 41000\n",
      "done 41100\n",
      "done 41200\n",
      "done 41300\n",
      "done 41400\n",
      "done 41500\n",
      "done 41600\n",
      "done 41700\n",
      "done 41800\n",
      "done 41900\n",
      "done 42000\n",
      "done 42100\n",
      "done 42200\n",
      "done 42300\n",
      "done 42400\n",
      "done 42500\n",
      "done 42600\n",
      "done 42700\n",
      "done 42800\n",
      "done 42900\n",
      "done 43000\n",
      "done 43100\n",
      "done 43200\n",
      "done 43300\n",
      "done 43400\n",
      "done 43500\n",
      "done 43600\n",
      "done 43700\n",
      "done 43800\n",
      "done 43900\n",
      "done 44000\n",
      "done 44100\n",
      "done 44200\n",
      "done 44300\n",
      "done 44400\n",
      "done 44500\n",
      "done 44600\n",
      "done 44700\n",
      "done 44800\n",
      "done 44900\n",
      "done 45000\n",
      "done 45100\n",
      "done 45200\n",
      "done 45300\n",
      "done 45400\n",
      "done 45500\n",
      "done 45600\n",
      "done 45700\n",
      "done 45800\n",
      "done 45900\n",
      "done 46000\n",
      "done 46100\n",
      "done 46200\n",
      "done 46300\n",
      "done 46400\n",
      "done 46500\n",
      "done 46600\n",
      "done 46700\n",
      "done 46800\n",
      "done 46900\n",
      "done 47000\n",
      "done 47100\n",
      "done 47200\n",
      "done 47300\n",
      "done 47400\n",
      "done 47500\n",
      "done 47600\n",
      "done 47700\n",
      "done 47800\n",
      "done 47900\n",
      "done 48000\n",
      "done 48100\n",
      "done 48200\n",
      "done 48300\n",
      "done 48400\n",
      "done 48500\n",
      "done 48600\n",
      "done 48700\n",
      "done 48800\n",
      "done 48900\n",
      "done 49000\n",
      "done 49100\n",
      "done 49200\n",
      "done 49300\n",
      "done 49400\n",
      "done 49500\n",
      "done 49600\n",
      "done 49700\n",
      "done 49800\n",
      "done 49900\n",
      "done 50000\n",
      "done 50100\n",
      "done 50200\n",
      "done 50300\n",
      "done 50400\n",
      "done 50500\n",
      "done 50600\n",
      "done 50700\n",
      "done 50800\n",
      "done 50900\n",
      "done 51000\n",
      "done 51100\n",
      "done 51200\n",
      "done 51300\n",
      "done 51400\n",
      "done 51500\n",
      "done 51600\n",
      "done 51700\n",
      "done 51800\n",
      "done 51900\n",
      "done 52000\n",
      "done 52100\n",
      "done 52200\n",
      "done 52300\n",
      "done 52400\n",
      "done 52500\n",
      "done 52600\n",
      "done 52700\n",
      "done 52800\n",
      "done 52900\n",
      "done 53000\n",
      "done 53100\n",
      "done 53200\n",
      "done 53300\n",
      "done 53400\n",
      "done 53500\n",
      "done 53600\n",
      "done 53700\n",
      "done 53800\n",
      "done 53900\n",
      "done 54000\n",
      "done 54100\n",
      "done 54200\n",
      "done 54300\n",
      "done 54400\n",
      "done 54500\n",
      "done 54600\n",
      "done 54700\n",
      "done 54800\n",
      "done 54900\n",
      "done 55000\n",
      "done 55100\n",
      "done 55200\n",
      "done 55300\n",
      "done 55400\n",
      "done 55500\n",
      "done 55600\n",
      "done 55700\n",
      "done 55800\n",
      "done 55900\n",
      "done 56000\n",
      "done 56100\n",
      "done 56200\n",
      "done 56300\n",
      "done 56400\n",
      "done 56500\n",
      "done 56600\n",
      "done 56700\n",
      "done 56800\n",
      "done 56900\n",
      "done 57000\n",
      "done 57100\n",
      "done 57200\n",
      "done 57300\n",
      "done 57400\n",
      "done 57500\n",
      "done 57600\n",
      "done 57700\n",
      "done 57800\n",
      "done 57900\n",
      "done 58000\n",
      "done 58100\n",
      "done 58200\n",
      "done 58300\n",
      "done 58400\n",
      "done 58500\n",
      "done 58600\n",
      "done 58700\n",
      "done 58800\n",
      "done 58900\n",
      "done 59000\n",
      "done 59100\n",
      "done 59200\n",
      "done 59300\n",
      "done 59400\n",
      "done 59500\n",
      "done 59600\n",
      "done 59700\n",
      "done 59800\n",
      "done 59900\n",
      "done 60000\n",
      "done 60100\n",
      "done 60200\n",
      "done 60300\n",
      "done 60400\n",
      "done 60500\n",
      "done 60600\n",
      "done 60700\n",
      "done 60800\n",
      "done 60900\n",
      "done 61000\n",
      "done 61100\n",
      "done 61200\n",
      "done 61300\n",
      "done 61400\n",
      "done 61500\n",
      "done 61600\n",
      "done 61700\n",
      "done 61800\n",
      "done 61900\n",
      "done 62000\n",
      "done 62100\n",
      "done 62200\n",
      "done 62300\n",
      "done 62400\n",
      "done 62500\n",
      "done 62600\n",
      "done 62700\n",
      "done 62800\n",
      "done 62900\n",
      "done 63000\n",
      "done 63100\n",
      "done 63200\n",
      "done 63300\n",
      "done 63400\n",
      "done 63500\n",
      "done 63600\n",
      "done 63700\n",
      "done 63800\n",
      "done 63900\n",
      "done 64000\n",
      "done 64100\n",
      "done 64200\n",
      "done 64300\n",
      "done 64400\n",
      "done 64500\n",
      "done 64600\n",
      "done 64700\n",
      "done 64800\n",
      "done 64900\n",
      "done 65000\n",
      "done 65100\n",
      "done 65200\n",
      "done 65300\n",
      "done 65400\n",
      "done 65500\n",
      "done 65600\n",
      "done 65700\n",
      "done 65800\n",
      "done 65900\n",
      "done 66000\n",
      "done 66100\n",
      "done 66200\n",
      "done 66300\n",
      "done 66400\n",
      "done 66500\n",
      "done 66600\n",
      "done 66700\n",
      "done 66800\n",
      "done 66900\n",
      "done 67000\n",
      "done 67100\n",
      "done 67200\n",
      "done 67300\n",
      "done 67400\n",
      "done 67500\n",
      "done 67600\n",
      "done 67700\n",
      "done 67800\n",
      "done 67900\n",
      "done 68000\n",
      "done 68100\n",
      "done 68200\n",
      "done 68300\n",
      "done 68400\n",
      "done 68500\n",
      "done 68600\n",
      "done 68700\n",
      "done 68800\n",
      "done 68900\n",
      "done 69000\n",
      "done 69100\n",
      "done 69200\n",
      "done 69300\n",
      "done 69400\n",
      "done 69500\n",
      "done 69600\n",
      "done 69700\n",
      "done 69800\n",
      "done 69900\n",
      "done 70000\n",
      "done 70100\n",
      "done 70200\n",
      "done 70300\n",
      "done 70400\n",
      "done 70500\n",
      "done 70600\n",
      "done 70700\n",
      "done 70800\n",
      "done 70900\n",
      "done 71000\n",
      "done 71100\n",
      "done 71200\n",
      "done 71300\n",
      "done 71400\n",
      "done 71500\n",
      "done 71600\n",
      "done 71700\n",
      "done 71800\n",
      "done 71900\n",
      "done 72000\n",
      "done 72100\n",
      "done 72200\n",
      "done 72300\n",
      "done 72400\n",
      "done 72500\n",
      "done 72600\n",
      "done 72700\n",
      "done 72800\n",
      "done 72900\n",
      "done 73000\n",
      "done 73100\n",
      "done 73200\n",
      "done 73300\n",
      "done 73400\n",
      "done 73500\n",
      "done 73600\n",
      "done 73700\n",
      "done 73800\n",
      "done 73900\n",
      "done 74000\n",
      "done 74100\n",
      "done 74200\n",
      "done 74300\n",
      "done 74400\n",
      "done 74500\n",
      "done 74600\n",
      "done 74700\n",
      "done 74800\n",
      "done 74900\n",
      "done 75000\n",
      "done 75100\n",
      "done 75200\n",
      "done 75300\n",
      "done 75400\n",
      "done 75500\n",
      "done 75600\n",
      "done 75700\n",
      "done 75800\n",
      "done 75900\n",
      "done 76000\n",
      "done 76100\n",
      "done 76200\n",
      "done 76300\n",
      "done 76400\n",
      "done 76500\n",
      "done 76600\n",
      "done 76700\n",
      "done 76800\n",
      "done 76900\n",
      "done 77000\n",
      "done 77100\n",
      "done 77200\n",
      "done 77300\n",
      "done 77400\n",
      "done 77500\n",
      "done 77600\n",
      "done 77700\n",
      "done 77800\n",
      "done 77900\n",
      "done 78000\n",
      "done 78100\n",
      "done 78200\n",
      "done 78300\n",
      "done 78400\n",
      "done 78500\n",
      "done 78600\n",
      "done 78700\n",
      "done 78800\n",
      "done 78900\n",
      "done 79000\n",
      "done 79100\n",
      "done 79200\n",
      "done 79300\n",
      "done 79400\n",
      "done 79500\n",
      "done 79600\n",
      "done 79700\n",
      "done 79800\n",
      "done 79900\n",
      "done 80000\n",
      "done 80100\n",
      "done 80200\n",
      "done 80300\n",
      "done 80400\n",
      "done 80500\n",
      "done 80600\n",
      "done 80700\n",
      "done 80800\n",
      "done 80900\n",
      "done 81000\n",
      "done 81100\n",
      "done 81200\n",
      "done 81300\n",
      "done 81400\n",
      "done 81500\n",
      "done 81600\n",
      "done 81700\n",
      "done 81800\n",
      "done 81900\n",
      "done 82000\n",
      "done 82100\n",
      "done 82200\n",
      "done 82300\n",
      "done 82400\n",
      "done 82500\n",
      "done 82600\n",
      "done 82700\n",
      "done 82800\n",
      "done 82900\n",
      "done 83000\n",
      "done 83100\n",
      "done 83200\n",
      "done 83300\n",
      "done 83400\n",
      "done 83500\n",
      "done 83600\n",
      "done 83700\n",
      "done 83800\n",
      "done 83900\n",
      "done 84000\n",
      "done 84100\n",
      "done 84200\n",
      "done 84300\n",
      "done 84400\n",
      "done 84500\n",
      "done 84600\n",
      "done 84700\n",
      "done 84800\n",
      "done 84900\n",
      "done 85000\n",
      "done 85100\n",
      "done 85200\n",
      "done 85300\n",
      "done 85400\n",
      "done 85500\n",
      "done 85600\n",
      "done 85700\n",
      "done 85800\n",
      "done 85900\n",
      "done 86000\n",
      "done 86100\n",
      "done 86200\n",
      "done 86300\n",
      "done 86400\n",
      "done 86500\n",
      "done 86600\n",
      "done 86700\n",
      "done 86800\n",
      "done 86900\n",
      "done 87000\n",
      "done 87100\n",
      "done 87200\n",
      "done 87300\n",
      "done 87400\n",
      "done 87500\n",
      "done 87600\n",
      "done 87700\n",
      "done 87800\n",
      "done 87900\n",
      "done 88000\n",
      "done 88100\n",
      "done 88200\n",
      "done 88300\n",
      "done 88400\n",
      "done 88500\n",
      "done 88600\n",
      "done 88700\n",
      "done 88800\n",
      "done 88900\n",
      "done 89000\n",
      "done 89100\n",
      "done 89200\n",
      "done 89300\n",
      "done 89400\n",
      "done 89500\n",
      "done 89600\n",
      "done 89700\n",
      "done 89800\n",
      "done 89900\n",
      "done 90000\n",
      "done 90100\n",
      "done 90200\n",
      "done 90300\n",
      "done 90400\n",
      "done 90500\n",
      "done 90600\n",
      "done 90700\n",
      "done 90800\n",
      "done 90900\n",
      "done 91000\n",
      "done 91100\n",
      "done 91200\n",
      "done 91300\n",
      "done 91400\n",
      "done 91500\n",
      "done 91600\n",
      "done 91700\n",
      "done 91800\n",
      "done 91900\n",
      "done 92000\n",
      "done 92100\n",
      "done 92200\n",
      "done 92300\n",
      "done 92400\n",
      "done 92500\n",
      "done 92600\n",
      "done 92700\n",
      "done 92800\n",
      "done 92900\n",
      "done 93000\n",
      "done 93100\n",
      "done 93200\n",
      "done 93300\n",
      "done 93400\n",
      "done 93500\n",
      "done 93600\n",
      "done 93700\n",
      "done 93800\n",
      "done 93900\n",
      "done 94000\n",
      "done 94100\n",
      "done 94200\n",
      "done 94300\n",
      "done 94400\n",
      "done 94500\n",
      "done 94600\n",
      "done 94700\n",
      "done 94800\n",
      "done 94900\n",
      "done 95000\n",
      "done 95100\n",
      "done 95200\n",
      "done 95300\n",
      "done 95400\n",
      "done 95500\n",
      "done 95600\n",
      "done 95700\n",
      "done 95800\n",
      "done 95900\n",
      "done 96000\n",
      "done 96100\n",
      "done 96200\n",
      "done 96300\n",
      "done 96400\n",
      "done 96500\n",
      "done 96600\n",
      "done 96700\n",
      "done 96800\n",
      "done 96900\n",
      "done 97000\n",
      "done 97100\n",
      "done 97200\n",
      "done 97300\n",
      "done 97400\n",
      "done 97500\n",
      "done 97600\n",
      "done 97700\n",
      "done 97800\n",
      "done 97900\n",
      "done 98000\n",
      "done 98100\n",
      "done 98200\n",
      "done 98300\n",
      "done 98400\n",
      "done 98500\n",
      "done 98600\n",
      "done 98700\n",
      "done 98800\n",
      "done 98900\n",
      "done 99000\n",
      "done 99100\n",
      "done 99200\n",
      "done 99300\n",
      "done 99400\n",
      "done 99500\n",
      "done 99600\n",
      "done 99700\n",
      "done 99800\n",
      "done 99900\n",
      "done 100000\n",
      "done 100100\n",
      "done 100200\n",
      "done 100300\n",
      "done 100400\n",
      "done 100500\n",
      "done 100600\n",
      "done 100700\n",
      "done 100800\n",
      "done 100900\n",
      "done 101000\n",
      "done 101100\n",
      "done 101200\n",
      "done 101300\n",
      "done 101400\n",
      "done 101500\n",
      "done 101600\n",
      "done 101700\n",
      "done 101800\n",
      "done 101900\n",
      "done 102000\n",
      "done 102100\n",
      "done 102200\n",
      "done 102300\n",
      "done 102400\n",
      "done 102500\n",
      "done 102600\n",
      "done 102700\n",
      "done 102800\n",
      "done 102900\n",
      "done 103000\n",
      "done 103100\n",
      "done 103200\n",
      "done 103300\n",
      "done 103400\n",
      "done 103500\n",
      "done 103600\n",
      "done 103700\n",
      "done 103800\n",
      "done 103900\n",
      "done 104000\n",
      "done 104100\n",
      "done 104200\n",
      "done 104300\n",
      "done 104400\n",
      "done 104500\n",
      "done 104600\n",
      "done 104700\n",
      "done 104800\n",
      "done 104900\n",
      "done 105000\n",
      "done 105100\n",
      "done 105200\n",
      "done 105300\n",
      "done 105400\n",
      "done 105500\n",
      "done 105600\n",
      "done 105700\n",
      "done 105800\n",
      "done 105900\n",
      "done 106000\n",
      "done 106100\n",
      "done 106200\n",
      "done 106300\n",
      "done 106400\n",
      "done 106500\n",
      "done 106600\n",
      "done 106700\n",
      "done 106800\n",
      "done 106900\n",
      "done 107000\n",
      "done 107100\n",
      "done 107200\n",
      "done 107300\n",
      "done 107400\n",
      "done 107500\n",
      "done 107600\n",
      "done 107700\n",
      "done 107800\n",
      "done 107900\n",
      "done 108000\n",
      "done 108100\n",
      "done 108200\n",
      "done 108300\n",
      "done 108400\n",
      "done 108500\n",
      "done 108600\n",
      "done 108700\n",
      "done 108800\n",
      "done 108900\n",
      "done 109000\n",
      "done 109100\n",
      "done 109200\n",
      "done 109300\n",
      "done 109400\n",
      "done 109500\n",
      "done 109600\n",
      "done 109700\n",
      "done 109800\n",
      "done 109900\n",
      "done 110000\n",
      "done 110100\n",
      "done 110200\n",
      "done 110300\n",
      "done 110400\n",
      "done 110500\n",
      "done 110600\n",
      "done 110700\n",
      "done 110800\n",
      "done 110900\n",
      "done 111000\n",
      "done 111100\n",
      "done 111200\n",
      "done 111300\n",
      "done 111400\n",
      "done 111500\n",
      "done 111600\n",
      "done 111700\n",
      "done 111800\n",
      "done 111900\n",
      "done 112000\n",
      "done 112100\n",
      "done 112200\n",
      "done 112300\n",
      "done 112400\n",
      "done 112500\n",
      "done 112600\n",
      "done 112700\n",
      "done 112800\n",
      "done 112900\n",
      "done 113000\n",
      "done 113100\n",
      "done 113200\n",
      "done 113300\n",
      "done 113400\n",
      "done 113500\n",
      "done 113600\n",
      "done 113700\n",
      "done 113800\n",
      "done 113900\n",
      "done 114000\n",
      "done 114100\n",
      "done 114200\n",
      "done 114300\n",
      "done 114400\n",
      "done 114500\n",
      "done 114600\n",
      "done 114700\n",
      "done 114800\n",
      "done 114900\n",
      "done 115000\n",
      "done 115100\n",
      "done 115200\n",
      "done 115300\n",
      "done 115400\n",
      "done 115500\n",
      "done 115600\n",
      "done 115700\n",
      "done 115800\n",
      "done 115900\n",
      "done 116000\n",
      "done 116100\n",
      "done 116200\n",
      "done 116300\n",
      "done 116400\n",
      "done 116500\n",
      "done 116600\n",
      "done 116700\n",
      "done 116800\n",
      "done 116900\n",
      "done 117000\n",
      "done 117100\n",
      "done 117200\n",
      "done 117300\n",
      "done 117400\n",
      "done 117500\n",
      "done 117600\n",
      "done 117700\n",
      "done 117800\n",
      "done 117900\n",
      "done 118000\n",
      "done 118100\n",
      "done 118200\n",
      "done 118300\n",
      "done 118400\n",
      "done 118500\n",
      "done 118600\n",
      "done 118700\n",
      "done 118800\n",
      "done 118900\n",
      "done 119000\n",
      "done 119100\n",
      "done 119200\n",
      "done 119300\n",
      "done 119400\n",
      "done 119500\n",
      "done 119600\n",
      "done 119700\n",
      "done 119800\n",
      "done 119900\n",
      "done 120000\n",
      "done 120100\n",
      "done 120200\n",
      "done 120300\n",
      "done 120400\n",
      "done 120500\n",
      "done 120600\n",
      "done 120700\n",
      "done 120800\n",
      "done 120900\n",
      "done 121000\n",
      "done 121100\n",
      "done 121200\n",
      "done 121300\n",
      "done 121400\n",
      "done 121500\n",
      "done 121600\n",
      "done 121700\n",
      "done 121800\n",
      "done 121900\n",
      "done 122000\n",
      "done 122100\n",
      "done 122200\n",
      "done 122300\n",
      "done 122400\n",
      "done 122500\n",
      "done 122600\n",
      "done 122700\n",
      "done 122800\n",
      "done 122900\n",
      "done 123000\n",
      "done 123100\n",
      "done 123200\n",
      "done 123300\n",
      "done 123400\n",
      "done 123500\n",
      "done 123600\n",
      "done 123700\n",
      "done 123800\n",
      "done 123900\n",
      "done 124000\n",
      "done 124100\n",
      "done 124200\n",
      "done 124300\n",
      "done 124400\n",
      "done 124500\n",
      "done 124600\n",
      "done 124700\n",
      "done 124800\n",
      "done 124900\n",
      "done 125000\n",
      "done 125100\n",
      "done 125200\n",
      "done 125300\n",
      "done 125400\n",
      "done 125500\n",
      "done 125600\n",
      "done 125700\n",
      "done 125800\n",
      "done 125900\n",
      "done 126000\n",
      "done 126100\n",
      "done 126200\n",
      "done 126300\n",
      "done 126400\n",
      "done 126500\n",
      "done 126600\n",
      "done 126700\n",
      "done 126800\n",
      "done 126900\n",
      "done 127000\n",
      "done 127100\n",
      "done 127200\n",
      "done 127300\n",
      "done 127400\n",
      "done 127500\n",
      "done 127600\n",
      "done 127700\n",
      "done 127800\n",
      "done 127900\n",
      "done 128000\n",
      "done 128100\n",
      "done 128200\n",
      "done 128300\n",
      "done 128400\n",
      "done 128500\n",
      "done 128600\n",
      "done 128700\n",
      "done 128800\n",
      "done 128900\n",
      "done 129000\n",
      "done 129100\n",
      "done 129200\n",
      "done 129300\n",
      "done 129400\n",
      "done 129500\n",
      "done 129600\n",
      "done 129700\n",
      "done 129800\n",
      "done 129900\n",
      "done 130000\n",
      "done 130100\n",
      "done 130200\n",
      "done 130300\n",
      "done 130400\n",
      "done 130500\n",
      "done 130600\n",
      "done 130700\n",
      "done 130800\n",
      "done 130900\n",
      "done 131000\n",
      "done 131100\n",
      "done 131200\n",
      "done 131300\n",
      "done 131400\n",
      "done 131500\n",
      "done 131600\n",
      "done 131700\n",
      "done 131800\n",
      "done 131900\n",
      "done 132000\n",
      "done 132100\n",
      "done 132200\n",
      "done 132300\n",
      "done 132400\n",
      "done 132500\n",
      "done 132600\n",
      "done 132700\n",
      "done 132800\n",
      "done 132900\n",
      "done 133000\n",
      "done 133100\n",
      "done 133200\n",
      "done 133300\n",
      "done 133400\n",
      "done 133500\n",
      "done 133600\n",
      "done 133700\n",
      "done 133800\n",
      "done 133900\n",
      "done 134000\n",
      "done 134100\n",
      "done 134200\n",
      "done 134300\n",
      "done 134400\n",
      "done 134500\n",
      "done 134600\n",
      "done 134700\n",
      "done 134800\n",
      "done 134900\n",
      "done 135000\n",
      "done 135100\n",
      "done 135200\n",
      "done 135300\n",
      "done 135400\n",
      "done 135500\n",
      "done 135600\n",
      "done 135700\n",
      "done 135800\n",
      "done 135900\n",
      "done 136000\n",
      "done 136100\n",
      "done 136200\n",
      "done 136300\n",
      "done 136400\n",
      "done 136500\n",
      "done 136600\n",
      "done 136700\n",
      "done 136800\n",
      "done 136900\n",
      "done 137000\n",
      "done 137100\n",
      "done 137200\n",
      "done 137300\n",
      "done 137400\n",
      "done 137500\n",
      "done 137600\n",
      "done 137700\n",
      "done 137800\n",
      "done 137900\n",
      "done 138000\n",
      "done 138100\n",
      "done 138200\n",
      "done 138300\n",
      "done 138400\n",
      "done 138500\n",
      "done 138600\n",
      "done 138700\n",
      "done 138800\n",
      "done 138900\n",
      "done 139000\n",
      "done 139100\n",
      "done 139200\n",
      "done 139300\n",
      "done 139400\n",
      "done 139500\n",
      "done 139600\n",
      "done 139700\n",
      "done 139800\n",
      "done 139900\n",
      "done 140000\n",
      "done 140100\n",
      "done 140200\n",
      "done 140300\n",
      "done 140400\n",
      "done 140500\n",
      "done 140600\n",
      "done 140700\n",
      "done 140800\n",
      "done 140900\n",
      "done 141000\n",
      "done 141100\n",
      "done 141200\n",
      "done 141300\n",
      "done 141400\n",
      "done 141500\n",
      "done 141600\n",
      "done 141700\n",
      "done 141800\n",
      "done 141900\n",
      "done 142000\n",
      "done 142100\n",
      "done 142200\n",
      "done 142300\n",
      "done 142400\n",
      "done 142500\n",
      "done 142600\n",
      "done 142700\n",
      "done 142800\n",
      "done 142900\n",
      "done 143000\n",
      "done 143100\n",
      "done 143200\n",
      "done 143300\n",
      "done 143400\n",
      "done 143500\n",
      "done 143600\n",
      "done 143700\n",
      "done 143800\n",
      "done 143900\n",
      "done 144000\n",
      "done 144100\n",
      "done 144200\n",
      "done 144300\n",
      "done 144400\n",
      "done 144500\n",
      "done 144600\n",
      "done 144700\n",
      "done 144800\n",
      "done 144900\n",
      "done 145000\n",
      "done 145100\n",
      "done 145200\n",
      "done 145300\n",
      "done 145400\n",
      "done 145500\n",
      "done 145600\n",
      "done 145700\n",
      "done 145800\n",
      "done 145900\n",
      "done 146000\n",
      "done 146100\n",
      "done 146200\n",
      "done 146300\n",
      "done 146400\n",
      "done 146500\n",
      "done 146600\n",
      "done 146700\n",
      "done 146800\n",
      "done 146900\n",
      "done 147000\n",
      "done 147100\n",
      "done 147200\n",
      "done 147300\n",
      "done 147400\n",
      "done 147500\n",
      "done 147600\n",
      "done 147700\n",
      "done 147800\n",
      "done 147900\n",
      "done 148000\n",
      "done 148100\n",
      "done 148200\n",
      "done 148300\n",
      "done 148400\n",
      "done 148500\n",
      "done 148600\n",
      "done 148700\n",
      "done 148800\n",
      "done 148900\n",
      "done 149000\n",
      "done 149100\n",
      "done 149200\n",
      "done 149300\n",
      "done 149400\n",
      "done 149500\n",
      "done 149600\n",
      "done 149700\n",
      "done 149800\n",
      "done 149900\n",
      "done 150000\n",
      "done 150100\n",
      "done 150200\n",
      "done 150300\n",
      "done 150400\n",
      "done 150500\n",
      "done 150600\n",
      "done 150700\n",
      "done 150800\n",
      "done 150900\n",
      "done 151000\n",
      "done 151100\n",
      "done 151200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "split=(0.8,0.1,0.1) #train,validation,test\n",
    "\n",
    "ORIGDATA = \"data/blitz.hdf5\"\n",
    "VALDATA = \"data/blitz_val.hdf5\"\n",
    "TESTDATA = \"data/blitz_test.hdf5\"\n",
    "OVERSAMPLEDPATH = \"data/blitz/\"\n",
    "\n",
    "#if the oversampled file doesn't exist, create it\n",
    "num_tensors = 0\n",
    "with h5py.File(ORIGDATA,\"r\",rdcc_nbytes=5*10**8) as f:\n",
    "    num_tensors = f[\"game_tensors\"].shape[0]\n",
    "\n",
    "    if not os.path.exists(OVERSAMPLEDPATH):\n",
    "        create_bins(f,0,int(num_tensors*split[0]),800,2500,OVERSAMPLEDPATH)\n",
    "    if not os.path.exists(VALDATA):\n",
    "        split_file(ORIGDATA,VALDATA,int(num_tensors*split[0]),int(num_tensors*(split[0]+split[1])))\n",
    "    if not os.path.exists(TESTDATA):\n",
    "        split_file(ORIGDATA,TESTDATA,int(num_tensors*(split[0]+split[1])),num_tensors)\n",
    "\n",
    "train_gen = TrainingGenerator(OVERSAMPLEDPATH,32,shuffle=False)\n",
    "val_gen = HDF5FileGenerator(VALDATA,32,shuffle=False)\n",
    "test_gen = HDF5FileGenerator(TESTDATA,32,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't read data (Invalid data for LZF decompression)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m stop_early \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      4\u001b[0m save \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodelO1.keras\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mstop_early\u001b[49m\u001b[43m,\u001b[49m\u001b[43msave\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mevaluate(test_gen)\n",
      "File \u001b[0;32m~/OneDrive - University of Aberdeen/ownCloud/scratch/chesspredict/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[7], line 43\u001b[0m, in \u001b[0;36mTrainingGenerator.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m num_items\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m#check if we need to load more data into the cache\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_index[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_file] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m#get the next element from the cache\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     x_batch\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame_cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_file][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_index[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_file]])\n",
      "Cell \u001b[0;32mIn[7], line 66\u001b[0m, in \u001b[0;36mTrainingGenerator.__load_data\u001b[0;34m(self, file_index)\u001b[0m\n\u001b[1;32m     64\u001b[0m     end_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mratings\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_indexes[file_index] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame_cache[file_index] \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgame_tensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrating_cache[file_index] \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mratings\u001b[39m\u001b[38;5;124m\"\u001b[39m][start_index:end_index]\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_indexes[file_index] \u001b[38;5;241m=\u001b[39m end_index\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/OneDrive - University of Aberdeen/ownCloud/scratch/chesspredict/.venv/lib/python3.12/site-packages/h5py/_hl/dataset.py:781\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fast_read_ok \u001b[38;5;129;01mand\u001b[39;00m (new_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 781\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fast_reader\u001b[38;5;241m.\u001b[39mread(args)\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall back to Python read pathway below\u001b[39;00m\n",
      "File \u001b[0;32mh5py/_selector.pyx:376\u001b[0m, in \u001b[0;36mh5py._selector.Reader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't read data (Invalid data for LZF decompression)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "save = tf.keras.callbacks.ModelCheckpoint('modelO1.keras', save_best_only=True,mode='auto',monitor='val_loss')\n",
    "\n",
    "model.fit(train_gen,validation_data=val_gen,epochs=100,callbacks=[stop_early,save])\n",
    "\n",
    "model.evaluate(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
