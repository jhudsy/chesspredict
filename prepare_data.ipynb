{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chess\n",
    "import chess.pgn\n",
    "from chess.engine import PovScore, Cp\n",
    "from io import StringIO,TextIOWrapper\n",
    "import h5py\n",
    "import sys,os\n",
    "import zstandard as zstd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the data in several stages.\n",
    "\n",
    "1. Extract valid games (see below) into pgn files using string operations.\n",
    "2. Create different hdf5 files for training, validation and testing containing (game,label) tensors for different time controls from multiple pgn files, filtering out invalid games using the python chess library.\n",
    "3. Create oversampled training files.\n",
    "\n",
    "Valid game files are based on the `TimeControl \"{a time control}\"` field, whether the term `[%eval` is in the string and whether `BlackRatingDiff` and `WhiteRatingDiff` is below a certain value as well as whether `Termination` is *not* `Abandoned` or `Rules nfraction`.\n",
    "\n",
    "If all the above conditions are met we utilise the `Board` class to parse the pgn, also checking that game length is above some minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stores a time-control:file string. The time-control is a regex string that matches the time-control of a game. The file string is the name of the file that the game should be saved to.\n",
    "\n",
    "file_dict = {\"300+0\":\"blitz\",\n",
    "             \"300+3\":\"blitz\",\n",
    "             \"60+0\":\"ultrabullet\",\n",
    "             \"120+1\":\"bullet\",\n",
    "             \"180+0\":\"superblitz\",\n",
    "             \"180+2\":\"superblitz\",\n",
    "             \"600+0\":\"rapid\",\n",
    "             \"600+5\":\"rapid\",\n",
    "             \"900+10\":\"rapid\"\n",
    "}\n",
    "\n",
    "#the maximum rating diff above which we ignore the game\n",
    "MAX_RATING_DIFF = 40\n",
    "\n",
    "#Termination strings that we ignore\n",
    "TERMINATION_STRINGS=set([\"Abandoned\",\"Rules infraction\"])\n",
    "\n",
    "NUM_MOVES = 40 #number of moves to consider for each game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_game_tensor(game_string):\n",
    "    \"\"\"returns a tensor representation of the game string. If the game is invalid, returns None. Note that a valid game will have 2 game tensors, one for each player. We also return the ratings of the players and the file that the game should be saved to.\"\"\"\n",
    "\n",
    "    #start by checking if the game is valid. The time control is a substring of the form 'TimeControl \"{TC}\"' where {TC} is a variable, check if {TC} is in the file_dict.\n",
    "\n",
    "    time_control = game_string.split('TimeControl \"')[1].split('\"')[0]\n",
    "    if time_control not in file_dict:\n",
    "        return None\n",
    "    \n",
    "    valid = False\n",
    "    \n",
    "    if '[%eval' in game_string and 'WhiteRatingDiff' in game_string and 'BlackRatingDiff' in game_string:\n",
    "        white_diff = int(game_string.split('WhiteRatingDiff \"')[1].split('\"')[0])\n",
    "        black_diff = int(game_string.split('BlackRatingDiff \"')[1].split('\"')[0])\n",
    "        if abs(white_diff) < MAX_RATING_DIFF and abs(black_diff) < MAX_RATING_DIFF:\n",
    "            valid = True\n",
    "    if not valid:\n",
    "        return None\n",
    "    \n",
    "    #check for termination strings\n",
    "    for term in TERMINATION_STRINGS:\n",
    "        if term in game_string:\n",
    "            return None\n",
    "    \n",
    "    ########prepare the game tensors\n",
    "    gt1 = np.zeros((NUM_MOVES,136),dtype=np.int16)\n",
    "    gt2 = np.zeros((NUM_MOVES,136),dtype=np.int16)\n",
    "\n",
    "    game = chess.pgn.read_game(StringIO(game_string))\n",
    "\n",
    "    board = game.board()\n",
    "    white_time = 0\n",
    "    black_time = 0\n",
    "\n",
    "    move_number = 0\n",
    "\n",
    "    current_eval = PovScore(Cp(0), chess.WHITE)\n",
    "    current_move_color = chess.WHITE\n",
    "    while True:\n",
    "        t = np.zeros(136)\n",
    "\n",
    "        for i in range(64):\n",
    "            if board.piece_at(i) is None:\n",
    "                t[i] = 0\n",
    "            elif board.piece_at(i).color == current_move_color:\n",
    "                t[i] = board.piece_at(i).piece_type\n",
    "            else:\n",
    "                t[i] = board.piece_at(i).piece_type + 7\n",
    "\n",
    "        # get the evaluation, time etc.\n",
    "        t[128] = move_number // 2  # move number\n",
    "\n",
    "        t[129] = white_time if current_move_color == chess.WHITE else black_time\n",
    "\n",
    "        t[131] = black_time if current_move_color == chess.WHITE else white_time\n",
    "\n",
    "        if current_eval is None: #mate in 0\n",
    "            t[135] = 1\n",
    "            t[134] = 0\n",
    "        elif current_eval.pov(current_move_color).is_mate(): #mate in X\n",
    "            t[133] = 1\n",
    "            t[132] = current_eval.pov(current_move_color).mate()\n",
    "        else:\n",
    "            t[133] = 0\n",
    "            t[132] = current_eval.pov(current_move_color).score()\n",
    "\n",
    "        if move_number == 0:\n",
    "            m = game.next()\n",
    "        else:\n",
    "            m = m.next()\n",
    "        if m is None:\n",
    "            break\n",
    "\n",
    "        if current_move_color == chess.WHITE:\n",
    "            white_time = m.clock()\n",
    "        else:\n",
    "            black_time = m.clock()\n",
    "\n",
    "        current_eval = m.eval()\n",
    "        board = m.board()\n",
    "\n",
    "        for i in range(64):\n",
    "            if board.piece_at(i) is None:\n",
    "                t[i + 64] = 0\n",
    "            elif board.piece_at(i).color == current_move_color:\n",
    "                t[i + 64] = board.piece_at(i).piece_type\n",
    "            else:\n",
    "                t[i + 64] = board.piece_at(i).piece_type + 7\n",
    "\n",
    "        t[130] = white_time if current_move_color == chess.WHITE else black_time\n",
    "\n",
    "        if current_eval is None:\n",
    "            t[135] = 1\n",
    "            t[134] = 0\n",
    "        elif current_eval.pov(current_move_color).is_mate():\n",
    "            t[135] = 1\n",
    "            t[134] = current_eval.pov(current_move_color).mate()\n",
    "        else:\n",
    "            t[135] = 0\n",
    "            t[134] = current_eval.pov(current_move_color).score()\n",
    "\n",
    "        if current_move_color == chess.WHITE:\n",
    "            gt1[move_number // 2] = t\n",
    "        else:\n",
    "            gt2[move_number // 2] = t\n",
    "\n",
    "        current_move_color = not current_move_color\n",
    "\n",
    "        move_number += 1\n",
    "\n",
    "        if move_number == NUM_MOVES * 2:\n",
    "            break\n",
    "\n",
    "    return np.array(gt1),np.array(gt2),int(game.headers['WhiteElo']),int(game.headers['BlackElo']),file_dict[time_control]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to read from an input file compressed using zst and write all the resultant game tensors and ratings to a hdf5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKSIZE = 1000\n",
    "\n",
    "def write_to_hdf5(reader):\n",
    "    \"\"\"writes the games in the reader to an hdf5 file. The reader is a generator that yields game strings. The games are stored in the file according to the time-control of the game. We will write the game tensors as a dataset in the file. We will also write the ratings of the players as a dataset in the file. The file will be named according to the time-control of the games.\"\"\"\n",
    "\n",
    "    #open all the files so that we don't have to keep doing it.\n",
    "    files = {}\n",
    "    for file_name in set(file_dict.values()):\n",
    "        files[file_name] = h5py.File(f\"{file_name}.hdf5\",\"a\") #5*10^8 bytes = 500MB for the cache for each file\n",
    "\n",
    "    file_indexes = {}\n",
    "    if files[file_name].get(\"game_tensors\") is not None:\n",
    "        file_indexes = {file_name:len(files[file_name][\"game_tensors\"]) for file_name in files}\n",
    "    else:\n",
    "        file_indexes = {file_name:0 for file_name in files}\n",
    "\n",
    "    game = \"\"\n",
    "    count = 0\n",
    "\n",
    "    for line in reader:\n",
    "        if line.startswith(\"[Event\") and game == \"\": #start of a new game when the file hasn't been initialized\n",
    "            game = line\n",
    "        elif line.startswith(\"[Event\") and game != \"\": #start of a new game when the file has been initialized, write the previous game to the file\n",
    "\n",
    "            if count % 1000 == 0:\n",
    "                print(\"read\",count,\"games\")\n",
    "            count += 1\n",
    "\n",
    "            game_tensors = get_game_tensor(game)\n",
    "            if game_tensors is None:\n",
    "                game = line\n",
    "                continue\n",
    "            else:\n",
    "                #print(\"read game\",game)\n",
    "                \n",
    "                gt1,gt2,white_rating,black_rating,file_name = game_tensors\n",
    "                #print(np.array(gt1.shape),np.array([white_rating]).shape)\n",
    "                f = files[file_name]\n",
    "                if f.get(\"game_tensors\") is None:\n",
    "                    f.create_dataset(\"game_tensors\",shape=(CHUNKSIZE,40,136),maxshape=(None,40,136),chunks=True,compression='lzf')#,compression_opts=1)\n",
    "                    f.create_dataset(\"ratings\",shape=(CHUNKSIZE,1),chunks=True,maxshape=(None,1))#,compression='gzip',compression_opts=9)\n",
    "                    f[\"game_tensors\"][0] = gt1\n",
    "                    f[\"game_tensors\"][1] = gt2\n",
    "                    f[\"ratings\"][0] = np.array([white_rating])\n",
    "                    f[\"ratings\"][1] = np.array([black_rating])\n",
    "                    file_indexes[file_name] = 2\n",
    "                else: #file already exists\n",
    "                    #check if we need to resize the dataset\n",
    "                    if file_indexes[file_name]+1 >= f[\"game_tensors\"].shape[0]:\n",
    "                        print(\"enlarging chunk for file\",file_name)\n",
    "                    #+1 as we are writing 2 games at a time\n",
    "                        f[\"game_tensors\"].resize((f[\"game_tensors\"].shape[0] + CHUNKSIZE,40,136))\n",
    "                        f[\"ratings\"].resize((f[\"ratings\"].shape[0] + CHUNKSIZE,1))\n",
    "                    #write the new game\n",
    "                    f[\"game_tensors\"][file_indexes[file_name]] = gt1\n",
    "                    f[\"game_tensors\"][file_indexes[file_name]+1] = gt2\n",
    "                    f[\"ratings\"][file_indexes[file_name]] = np.array([white_rating])\n",
    "                    f[\"ratings\"][file_indexes[file_name]+1] = np.array([black_rating])\n",
    "                    file_indexes[file_name] += 2\n",
    "                game = line\n",
    "        else: #continue reading the game\n",
    "            game += line\n",
    "\n",
    "    for file_name in files:\n",
    "        f = files[file_name]\n",
    "        #reshape the datasets to remove the extra space\n",
    "        f[\"game_tensors\"].resize((file_indexes[f],40,136))\n",
    "        f[\"ratings\"].resize((file_indexes[f],1))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow processing of \"plain\" files or \"zst\" files passed in from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(fn):\n",
    "    #if the filename ends in .pgn we will read it as a text file. If it ends in .zst we will read it as a compressed file using streaming.\n",
    "    if fn.endswith(\".pgn\"):\n",
    "        with open(fn,\"r\") as f:\n",
    "            write_to_hdf5(f)\n",
    "    elif fn.endswith(\".zst\"):\n",
    "        with open(fn,\"rb\") as f:\n",
    "            dctx = zstd.ZstdDecompressor()\n",
    "            with dctx.stream_reader(f) as reader:\n",
    "                text_stream = TextIOWrapper(reader, encoding='utf-8')\n",
    "                write_to_hdf5(text_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read_file(\"data/all_data/lichess09.pgn.zst\")\n",
    "read_file(\"data/all_data/lichess05.pgn.zst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_bins(f,start_index,end_index,min_rating,max_rating,path): \n",
    "    #read the hdf file up to some index and bins index values into num_bins bins based on the rating. These bins are in intervals of 50. We will store the data in the bins in separate files in the path directory.\n",
    "    #N.B., f is a h5py file object. We will create a set of files under path containing the data in the bins.\n",
    "\n",
    "    #if the path doesn't exist, create it\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    ratings = f[\"ratings\"][start_index:end_index]\n",
    "    \n",
    "    num_bins = int((max_rating//50)-(min_rating//50))\n",
    "    start_bin_rating = (min_rating//50)*50\n",
    "\n",
    "    files = [h5py.File(f\"{path}/bin_{i}.hdf5\",\"w\") for i in range(num_bins)]\n",
    "    for fl in files:\n",
    "        fl.create_dataset(\"game_tensors\",shape=(0,40,136),maxshape=(None,40,136),compression='lzf',chunks=True)\n",
    "        fl.create_dataset(\"ratings\",shape=(0,1),maxshape=(None,1),chunks=True)\n",
    "\n",
    "    for i in range(len(ratings)):\n",
    "        bin=0\n",
    "        r=f[\"ratings\"][i][0]\n",
    "        if r<=min_rating:\n",
    "            bin=0\n",
    "        elif r>=max_rating:\n",
    "            bin=num_bins-1\n",
    "        else:\n",
    "            bin=int((r-start_bin_rating)//50)\n",
    "        files[bin][\"game_tensors\"].resize((files[bin][\"game_tensors\"].shape[0]+1,40,136))\n",
    "        files[bin][\"ratings\"].resize((files[bin][\"ratings\"].shape[0]+1,1))\n",
    "        files[bin][\"game_tensors\"][-1] = f[\"game_tensors\"][i]\n",
    "        files[bin][\"ratings\"][-1] = f[\"ratings\"][i]\n",
    "        if i%100==0:\n",
    "            print(f\"done {i}\")\n",
    "    \n",
    "    for fl in files:\n",
    "        fl.close()\n",
    "\n",
    "####################################################\n",
    "#split_file(ORIGDATA,TESTDATA,int(num_tensors*(split[0]+split[1]),num_tensors))\n",
    "def split_file(original_file_path,new_file_path,start_index,end_index):\n",
    "    #splits the original file by extracting the data from start_index to end_index and writing it to a new file.\n",
    "    with h5py.File(original_file_path,\"r\") as f:\n",
    "        with h5py.File(new_file_path,\"w\") as nf:\n",
    "            nf.create_dataset(\"game_tensors\",data=f[\"game_tensors\"][start_index:end_index])\n",
    "            nf.create_dataset(\"ratings\",data=f[\"ratings\"][start_index:end_index])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import random\n",
    "\n",
    "class InMemoryOverSamplngGenerator(Sequence):\n",
    "    def __init__(self,path,batch_size,**kwargs):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = kwargs.get(\"shuffle\",True)\n",
    "        self.num_items = kwargs.get(\"num_items\",None)\n",
    "\n",
    "        self.files = [h5py.File(f\"{path}/bin_{i}.hdf5\",\"r\") for i in range(len(os.listdir(path)))]\n",
    "                      \n",
    "        self.bins = [[] for i in range(len(self.files))]\n",
    "        self.num_bins = len(self.bins)\n",
    "\n",
    "        #read the data into memory\n",
    "        for i in range(len(self.files)):\n",
    "            self.bins[i] = (self.files[i][\"game_tensors\"][:],self.files[i][\"ratings\"][:])\n",
    "        \n",
    "        self.current_bin = 0\n",
    "        self.bin_indexes = [0 for i in range(self.num_bins)]\n",
    "\n",
    "        for f in self.files:\n",
    "            f.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.num_items is None:\n",
    "            return sum([len(b[0]) for b in self.bins])//self.batch_size\n",
    "        else:\n",
    "            return self.num_items//self.batch_size\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        num_items = self.batch_size\n",
    "        while num_items > 0:\n",
    "            if self.bin_indexes[self.current_bin] == len(self.bins[self.current_bin][0]):\n",
    "                self.bin_indexes[self.current_bin] = 0\n",
    "                self.current_bin += 1\n",
    "                self.current_bin %= self.num_bins\n",
    "\n",
    "            x_batch.append(self.bins[self.current_bin][0][self.bin_indexes[self.current_bin]])\n",
    "            y_batch.append(self.bins[self.current_bin][1][self.bin_indexes[self.current_bin]])\n",
    "\n",
    "            self.bin_indexes[self.current_bin] += 1\n",
    "            num_items -= 1\n",
    "\n",
    "        return np.array(x_batch),np.array(y_batch)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            for i in range(self.num_bins):\n",
    "                state = random.random.get_state()\n",
    "                seed = random.randint(0,10000)\n",
    "                random.seed(seed)\n",
    "                random.shuffle(self.bins[i][0])\n",
    "                random.seed(seed)\n",
    "                random.shuffle(self.bins[i][1])\n",
    "                random.random.set_state(state)\n",
    "    \n",
    "\n",
    "\n",
    "class TrainingGenerator(Sequence):\n",
    "    \"\"\"This generator takes in a path containing a set of hdf5 files, each of which is a bin. The generator will yield data by taking batch_size elements from each bin in the path. We will store cache_size elements from each file in memory, loading them in as needed. The generator will load the data from the files in the path in order, and will loop back to the start when it reaches the end of the files. The generator will also shuffle the order of the files if shuffle is set to True.\"\"\"\n",
    "    def __init__(self,path,batch_size,**kwargs):\n",
    "                \n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = kwargs.get(\"shuffle\",True)\n",
    "        self.cache_size = kwargs.get(\"cache_size\",512)\n",
    "        self.num_items = kwargs.get(\"num_items\",None)\n",
    "        \n",
    "\n",
    "        self.files = [h5py.File(f\"{path}/bin_{i}.hdf5\",\"a\") for i in range(len(os.listdir(path)))]\n",
    "        self.num_files = len(self.files)\n",
    "        self.file_indexes = [0 for i in range(self.num_files)]\n",
    "        self.game_cache = [np.zeros((self.cache_size,40,136),dtype=np.int16) for i in range(self.num_files)]\n",
    "        self.rating_cache = [np.zeros((self.cache_size,1),dtype=np.int16) for i in range(self.num_files)]\n",
    "\n",
    "        self.cache_index = [0 for i in range(self.num_files)]\n",
    "\n",
    "        self.current_file = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"returns the number of batches in the generator. This is the sum of the number of elements in each file divided by the batch size.\"\"\"\n",
    "        if self.num_items is None:\n",
    "            return sum([len(f[\"ratings\"]) for f in self.files])//self.batch_size\n",
    "        else:\n",
    "            return self.num_items//self.batch_size\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        \"\"\"returns the next batch. The batch is a tuple containing the game tensors and the ratings. We ignore the index as we will just iterate through the files in order.\"\"\"\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        num_items = self.batch_size\n",
    "\n",
    "        while num_items>0:\n",
    "            #check if we need to load more data into the cache\n",
    "            if self.cache_index[self.current_file] == 0:\n",
    "                self.__load_data(self.current_file)\n",
    "\n",
    "            #get the next element from the cache\n",
    "            x_batch.append(self.game_cache[self.current_file][self.cache_index[self.current_file]])\n",
    "            y_batch.append(self.rating_cache[self.current_file][self.cache_index[self.current_file]])\n",
    "\n",
    "            self.cache_index[self.current_file] += 1\n",
    "            self.cache_index[self.current_file] %= self.cache_size\n",
    "            self.current_file += 1\n",
    "            self.current_file %= self.num_files\n",
    "            num_items -= 1\n",
    "\n",
    "        return np.array(x_batch),np.array(y_batch)\n",
    "    \n",
    "    def __load_data(self,file_index):\n",
    "        \"\"\"loads the next cache_size elements from the file at file_index into the cache. If the end of the file is reached, the cache loops around. \"\"\"\n",
    "        f = self.files[file_index]\n",
    "        start_index = self.file_indexes[file_index]\n",
    "        end_index = start_index + self.cache_size\n",
    "\n",
    "        #print(f\"reading from file {f.filename} from {start_index} to {end_index}, cache size is {self.cache_size}\")\n",
    "\n",
    "        if end_index > len(f[\"ratings\"]):\n",
    "            num_read = len(f[\"ratings\"]) - start_index\n",
    "            self.game_cache[file_index][:num_read] = f[\"game_tensors\"][start_index:]\n",
    "            self.rating_cache[file_index][:num_read] = f[\"ratings\"][start_index:]\n",
    "\n",
    "            #print(f\"I have read {num_read} elements from file, about to read {self.cache_size-num_read} elements from the start of the file\")\n",
    "\n",
    "            self.game_cache[file_index][num_read:] = f[\"game_tensors\"][:self.cache_size-num_read]\n",
    "            self.rating_cache[file_index][num_read:] = f[\"ratings\"][:self.cache_size-num_read]\n",
    "\n",
    "            self.file_indexes[file_index] = end_index % len(f[\"ratings\"])\n",
    "        else:\n",
    "            self.game_cache[file_index] = f[\"game_tensors\"][start_index:end_index]\n",
    "            self.rating_cache[file_index] = f[\"ratings\"][start_index:end_index]\n",
    "            self.file_indexes[file_index] = end_index\n",
    "    \n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"shuffles the order of the files if shuffle is set to True. Also cleares the caches and resets the file indexes.\"\"\"\n",
    "        self.file_indexes = [0 for i in range(self.num_files)]\n",
    "        self.game_cache = [np.zeros((self.cache_size,40,136),dtype=np.int16) for i in range(self.num_files)]\n",
    "        self.rating_cache = [np.zeros((self.cache_size,1),dtype=np.int16) for i in range(self.num_files)]\n",
    "        self.cache_index = [0 for i in range(self.num_files)]\n",
    "        self.current_file = 0\n",
    "\n",
    "        if self.shuffle:\n",
    "            print(\"shuffling files\")    \n",
    "            #save current random number generator state\n",
    "            prng_state = random.random.get_state()\n",
    "            \n",
    "            for f in self.files:\n",
    "                print(\"shuffling file \",f.filename)\n",
    "                seed = random.randint(0,10000)\n",
    "                random.seed(seed)\n",
    "                random.shuffle(f[\"ratings\"])\n",
    "                random.seed(seed)\n",
    "                random.shuffle(f[\"game_tensors\"])\n",
    "            \n",
    "            #restore the random number generator state\n",
    "            random.random.set_state(prng_state)\n",
    "            print(\"done shuffling files\")\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        for f in self.files:\n",
    "            f.close()\n",
    "\n",
    "class InMemoryGenerator(Sequence):\n",
    "    def __init__(self,file,batch_size,shuffle=False):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle=shuffle\n",
    "\n",
    "        with h5py.File(file,\"r\") as f:\n",
    "            self.game_tensors = f[\"game_tensors\"][:]\n",
    "            self.ratings = f[\"ratings\"][:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)//self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "        x_batch=[]\n",
    "        y_batch=[]\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            x_batch.append(self.game_tensors[(index*self.batch_size+i)%len(self.game_tensors)])\n",
    "            y_batch.append(self.ratings[(index*self.batch_size+i)%len(self.ratings)])\n",
    "        \n",
    "        return np.array(x_batch),np.array(y_batch)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            state = random.random.get_state()\n",
    "            seed = random.randint(0,10000)\n",
    "            random.seed(seed)\n",
    "            random.shuffle(self.game_tensors)\n",
    "            random.seed(seed)\n",
    "            random.shuffle(self.ratings)\n",
    "            random.random.set_state(state)\n",
    "\n",
    "        \n",
    "class HDF5FileGenerator(Sequence):\n",
    "    def __init__(self, file, batch_size, shuffle=False):\n",
    "        self.f = h5py.File(file,\"a\",rdcc_nbytes=5*10**8) #500MB cache\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle=shuffle\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.f[\"ratings\"]))//self.batch_size\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        x_batch=[]\n",
    "        y_batch=[]\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            x_batch.append(self.f[\"game_tensors\"][(index*self.batch_size+i)%len(self.f[\"game_tensors\"])])\n",
    "            y_batch.append(self.f[\"ratings\"][(index*self.batch_size+i)%len(self.f[\"ratings\"])])\n",
    "        \n",
    "        return np.array(x_batch),np.array(y_batch)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            state = random.random.get_state()\n",
    "            seed = random.randint(0,10000)\n",
    "            random.seed(seed)\n",
    "            random.shuffle(self.f[\"game_tensors\"])\n",
    "            random.seed(seed)\n",
    "            random.shuffle(self.f[\"ratings\"])\n",
    "            random.random.set_state(state)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "inputs = Input(shape=(NUM_MOVES, 136)) #full tensor\n",
    "\n",
    "#make a dense layer for each of the NUM_MOVES elements. The output of each dense layer is a 1D tensor of 137 elements. Each of these tensors is then concatenated to form a 2D tensor of 137xNUM_MOVES elements. This tensor is then fed into an LSTM layer.\n",
    "\n",
    "x = TimeDistributed(Dense(80,activation = 'relu'))(inputs)\n",
    "\n",
    "x = LSTM(40,return_sequences = True)(x)\n",
    "x = LSTM(32)(x)\n",
    "#x = LSTM(40)(x)\n",
    "x = Dense(60,activation='relu')(x)\n",
    "\n",
    "output = Dense(1,activation='relu',name=\"Elo\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs,outputs=[output])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                    loss={'Elo':'mae'},\n",
    "                    metrics={'Elo':'mae'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "split=(0.8,0.1,0.1) #train,validation,test\n",
    "\n",
    "ORIGDATA = \"data/blitz.hdf5\"\n",
    "VALDATA = \"data/blitz_val.hdf5\"\n",
    "TESTDATA = \"data/blitz_test.hdf5\"\n",
    "OVERSAMPLEDPATH = \"data/blitz/\"\n",
    "\n",
    "#if the oversampled file doesn't exist, create it\n",
    "num_tensors = 0\n",
    "with h5py.File(ORIGDATA,\"r\",rdcc_nbytes=5*10**8) as f:\n",
    "    num_tensors = f[\"game_tensors\"].shape[0]\n",
    "\n",
    "    if not os.path.exists(OVERSAMPLEDPATH):\n",
    "        create_bins(f,0,int(num_tensors*split[0]),800,2500,OVERSAMPLEDPATH)\n",
    "    if not os.path.exists(VALDATA):\n",
    "        split_file(ORIGDATA,VALDATA,int(num_tensors*split[0]),int(num_tensors*(split[0]+split[1])))\n",
    "    if not os.path.exists(TESTDATA):\n",
    "        split_file(ORIGDATA,TESTDATA,int(num_tensors*(split[0]+split[1])),num_tensors)\n",
    "\n",
    "train_gen = TrainingGenerator(OVERSAMPLEDPATH,32,shuffle=False)\n",
    "val_gen = HDF5FileGenerator(VALDATA,32,shuffle=False)\n",
    "test_gen = HDF5FileGenerator(TESTDATA,32,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "save = tf.keras.callbacks.ModelCheckpoint('modelO1.keras', save_best_only=True,mode='auto',monitor='val_loss')\n",
    "\n",
    "model.fit(train_gen,validation_data=val_gen,epochs=100,callbacks=[stop_early,save])\n",
    "\n",
    "model.evaluate(test_gen)\n",
    "\n",
    "#N.B., the model saved with the save callback is the best model according to the validation loss. We can load this model and evaluate it on the test data.\n",
    "\n",
    "model = keras.models.load_model('modelO1.keras')\n",
    "model.evaluate(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "\n",
    "    #inputs = Input(shape=(NUM_MOVES, 132)) #if no eval is used\n",
    "    inputs = Input(shape=(NUM_MOVES, 136)) #full tensor\n",
    "    #inputs = Input(shape=(NUM_MOVES,8)) #if only the eval is used\n",
    "    \n",
    "    x = inputs\n",
    "\n",
    "    #prepare hyperparameter tuning\n",
    "\n",
    "    num_LSTM_layers = hp.Int('num_LSTM_layers',0,3)\n",
    "    num_LSTM_units=[]\n",
    "    for i in range(num_LSTM_layers):\n",
    "        num_LSTM_units.append(hp.Int('lstm'+str(i+1)+'_units',\n",
    "                                     min_value = 32,\n",
    "                                     max_value = 64,\n",
    "                                     step=8))\n",
    "        \n",
    "                                     \n",
    "    num_dense_layers = hp.Int('num_dense_layers',1,3)\n",
    "    num_dense_units = []\n",
    "    dense_activation = []\n",
    "\n",
    "    for i in range(num_dense_layers):\n",
    "        num_dense_units.append(hp.Int('dense'+str(i+1)+'_units',\n",
    "                                     min_value = 32,\n",
    "                                     max_value = 128,\n",
    "                                     step=16))\n",
    "        dense_activation.append(hp.Choice(\"dense\"+str(i+1)+\"_activation\",[\"relu\", \"leaky_relu\"]))\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-2])\n",
    "\n",
    "    #make the NN\n",
    "    x = TimeDistributed(Dense(hp.Int('td_dense_units',min_value=32,max_value=128,step=16),activation=hp.Choice(\"td_dense_activation\",[\"relu\",\"leaky_relu\"])))(x)\n",
    "\n",
    "    for i in range(num_LSTM_layers):\n",
    "        x = LSTM(num_LSTM_units[i],return_sequences=True if i<num_LSTM_layers else False)(x)\n",
    "\n",
    "    \n",
    "    for i in range(num_dense_layers):\n",
    "        x = Dense(num_dense_units[i],activation = dense_activation[i])(x)\n",
    "\n",
    "\n",
    "    output = Dense(1,activation='relu',name=\"Elo\")(x)\n",
    "    \n",
    "\n",
    "    #Alternative: set outputs to be hot encoded between 48 values\n",
    "    #output1 = Dense(48,activation='softmax',name=\"WhiteElo\")(x)\n",
    "    #output2 = Dense(48,activation='softmax',name=\"BlackElo\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs,outputs=[output])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                    loss={'Elo':'mae'},\n",
    "                    metrics={'Elo':'mae'})\n",
    "\n",
    "    return model\n",
    "\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_loss',\n",
    "                     max_epochs=100,\n",
    "                     factor=5)\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "save = tf.keras.callbacks.ModelCheckpoint('modelCP.keras', save_best_only=True,mode='auto',monitor='val_loss')\n",
    "\n",
    "tuner.search(train_gen,validation_data=val_gen,epochs=100,callbacks=[stop_early,save])\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(best_hps.values)\n",
    "\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "model.fit(train_gen,validation_data=val_gen,epochs=100,callbacks=[stop_early,save])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
